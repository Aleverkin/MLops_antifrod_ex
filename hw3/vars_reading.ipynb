{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d559dc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 316.9 MB 7.7 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 71.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425367 sha256=a4aa2df9b3afc1ee34fabcbe56e0c038ec60a4e3bba4688363ebcb046ee2c2c0\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a6/ce/f9/17d82c92f044018df2fe30af63ac043447720d5b2cee39b40f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark\n",
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38ec89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4676a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"OTUS\")\n",
    "        .config('spark.executor.cores', '2')\n",
    "        .config('spark.executor.instances', '4')\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "sql = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd54bbb",
   "metadata": {},
   "source": [
    "### Split files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8762c275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-08-22.txt', '2019-09-21.txt', '2019-10-21.txt']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name_files = [x.split(' ')[-1].split('/')[-1] for x in files.split('\\n')]\n",
    "# name_files.remove('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ec9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"name_files.pickle\", \"rb\") as output_file:\n",
    "    name_files = pickle.load(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb0867",
   "metadata": {},
   "source": [
    "# Вариант чтения 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ea57ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://rc1a-dataproc-m-skgoxg37ozdzyuje.mdb.yandexcloud.net'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultFS = spark._jsc.hadoopConfiguration().get(\"fs.defaultFS\")\n",
    "defaultFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c260e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = spark.read.format('csv').options(header='true', inferSchema='true').load(defaultFS + '/user/hive/warehouse/2019-08-22.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a6725c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# tranaction_id | tx_datetime | customer_id | terminal_id | tx_amount | tx_time_seconds | tx_time_days | tx_fraud | tx_fraud_scenario']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a5868",
   "metadata": {},
   "source": [
    "# Вариант чтения 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80a9759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "def read_txt_files(name):\n",
    "    data = spark.read.text(\"s3a://hw2-mlops-course/\" + name)\n",
    "\n",
    "    name_cols = ['tranaction_id',\n",
    "                 'tx_datetime',\n",
    "                 'customer_id',\n",
    "                 'terminal_id',\n",
    "                 'tx_amount',\n",
    "                 'tx_time_seconds',\n",
    "                 'tx_time_days',\n",
    "                 'tx_fraud',\n",
    "                 'tx_fraud_scenario']\n",
    "\n",
    "    data = data.withColumn('temp', f.split('value', ','))\\\n",
    "                .select(*(f.col('temp').getItem(i).alias(name_col) for i, name_col in enumerate(name_cols)))\\\n",
    "                .filter(f.col('tx_datetime').isNotNull())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1258cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_file in name_files:\n",
    "    df = read_txt_files(name_file).limit(10000)\n",
    "    \n",
    "    clear_name = name_file.split('.')[0]\n",
    "    \n",
    "    #df.write.mode('overwrite').parquet('/user/hive/warehouse/' + clear_name + '.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e3273",
   "metadata": {},
   "source": [
    "# Вариант чтения 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "struct = StructType([\n",
    "    StructField(\"tranaction_id\", IntegerType(), nullable = True),\n",
    "    StructField(\"tx_datetime\", StringType(), nullable = True), \n",
    "    StructField(\"customer_id\", IntegerType(), nullable = True), \n",
    "    StructField(\"terminal_id\", IntegerType(), nullable = True),\n",
    "    StructField(\"tx_amount\", DoubleType(), nullable = True), \n",
    "    StructField(\"tx_time_seconds\", IntegerType(), nullable = True), \n",
    "    StructField(\"tx_time_days\", IntegerType(), nullable = True),\n",
    "    StructField(\"tx_fraud\", IntegerType(), nullable = True), \n",
    "    StructField(\"tx_fraud_scenario\", IntegerType(), nullable = True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(name):\n",
    "    df = spark.read\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .option(\"inferSchema\", \"false\")\\\n",
    "          .option(\"delimiter\", \",\")\\\n",
    "          .schema(struct)\\\n",
    "          .csv(\"s3a://hw2-mlops-course/\" + name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e18ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a3c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f2540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9682b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
