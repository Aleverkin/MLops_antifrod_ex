{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d559dc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 316.9 MB 7.7 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 71.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425367 sha256=a4aa2df9b3afc1ee34fabcbe56e0c038ec60a4e3bba4688363ebcb046ee2c2c0\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a6/ce/f9/17d82c92f044018df2fe30af63ac043447720d5b2cee39b40f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark\n",
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38ec89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4676a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"OTUS\")\n",
    "        .config('spark.executor.cores', '2')\n",
    "        .config('spark.executor.instances', '4')\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "sql = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd54bbb",
   "metadata": {},
   "source": [
    "### Split files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721417b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \"\"\"2023-10-10 18:24:15 2807409271 2019-08-22.txt\n",
    "2023-10-10 18:23:34 2854479008 2019-09-21.txt\n",
    "2023-10-10 18:23:35 2895460543 2019-10-21.txt\n",
    "2023-10-10 18:23:34 2939120942 2019-11-20.txt\n",
    "2023-10-10 18:23:35 2995462277 2019-12-20.txt\n",
    "2023-10-10 18:24:37 2994906767 2020-01-19.txt\n",
    "2023-10-10 18:24:39 2995431240 2020-02-18.txt\n",
    "2023-10-10 18:24:39 2995176166 2020-03-19.txt\n",
    "2023-10-10 18:24:39 2996034632 2020-04-18.txt\n",
    "2023-10-10 18:25:40 2995666965 2020-05-18.txt\n",
    "2023-10-10 18:25:48 2994699401 2020-06-17.txt\n",
    "2023-10-10 18:25:48 2995810010 2020-07-17.txt\n",
    "2023-10-10 18:25:46 2995995152 2020-08-16.txt\n",
    "2023-10-10 18:25:48 2995778382 2020-09-15.txt\n",
    "2023-10-10 18:26:31 2995868596 2020-10-15.txt\n",
    "2023-10-10 18:26:53 2995467533 2020-11-14.txt\n",
    "2023-10-10 18:26:52 2994761624 2020-12-14.txt\n",
    "2023-10-10 18:26:54 2995390576 2021-01-13.txt\n",
    "2023-10-10 18:26:53 2995780517 2021-02-12.txt\n",
    "2023-10-10 18:27:35 2995191659 2021-03-14.txt\n",
    "2023-10-10 18:27:56 2995446495 2021-04-13.txt\n",
    "2023-10-10 18:27:58 3029170975 2021-05-13.txt\n",
    "2023-10-10 18:28:01 3042691991 2021-06-12.txt\n",
    "2023-10-10 18:28:02 3041980335 2021-07-12.txt\n",
    "2023-10-10 18:28:45 3042662187 2021-08-11.txt\n",
    "2023-10-10 18:29:06 3042455173 2021-09-10.txt\n",
    "2023-10-10 18:29:08 3042424238 2021-10-10.txt\n",
    "2023-10-10 18:29:09 3042358698 2021-11-09.txt\n",
    "2023-10-10 18:29:07 3042923985 2021-12-09.txt\n",
    "2023-10-10 18:29:52 3042868087 2022-01-08.txt\n",
    "2023-10-10 18:30:13 3043148790 2022-02-07.txt\n",
    "2023-10-10 18:30:15 3042312191 2022-03-09.txt\n",
    "2023-10-10 18:30:15 3041973966 2022-04-08.txt\n",
    "2023-10-10 18:30:19 3073760161 2022-05-08.txt\n",
    "2023-10-10 18:31:02 3089378246 2022-06-07.txt\n",
    "2023-10-10 18:31:23 3089589719 2022-07-07.txt\n",
    "2023-10-10 18:31:25 3090000257 2022-08-06.txt\n",
    "2023-10-10 18:31:22 3089390874 2022-09-05.txt\n",
    "2023-10-10 18:31:25 3109468067 2022-10-05.txt\n",
    "2023-10-10 18:31:35 3136657969 2022-11-04.txt\n",
    "2023-10-10 18:21:44     145611 train.csv\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8762c275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-08-22.txt', '2019-09-21.txt', '2019-10-21.txt']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_files = [x.split(' ')[-1].split('/')[-1] for x in files.split('\\n')]\n",
    "name_files.remove('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb0867",
   "metadata": {},
   "source": [
    "# Вариант чтения 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ea57ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://rc1a-dataproc-m-skgoxg37ozdzyuje.mdb.yandexcloud.net'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultFS = spark._jsc.hadoopConfiguration().get(\"fs.defaultFS\")\n",
    "defaultFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c260e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = spark.read.format('csv').options(header='true', inferSchema='true').load(defaultFS + '/user/hive/warehouse/2019-08-22.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a6725c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# tranaction_id | tx_datetime | customer_id | terminal_id | tx_amount | tx_time_seconds | tx_time_days | tx_fraud | tx_fraud_scenario']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a5868",
   "metadata": {},
   "source": [
    "# Вариант чтения 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80a9759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "def read_txt_files(name):\n",
    "    data = spark.read.text(\"s3a://hw2-mlops-course/\" + name)\n",
    "\n",
    "    name_cols = ['tranaction_id',\n",
    "                 'tx_datetime',\n",
    "                 'customer_id',\n",
    "                 'terminal_id',\n",
    "                 'tx_amount',\n",
    "                 'tx_time_seconds',\n",
    "                 'tx_time_days',\n",
    "                 'tx_fraud',\n",
    "                 'tx_fraud_scenario']\n",
    "\n",
    "    data = data.withColumn('temp', f.split('value', ','))\\\n",
    "                .select(*(f.col('temp').getItem(i).alias(name_col) for i, name_col in enumerate(name_cols)))\\\n",
    "                .filter(f.col('tx_datetime').isNotNull())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1258cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_file in name_files:\n",
    "    df = read_txt_files(name_file).limit(10000)\n",
    "    \n",
    "    clear_name = name_file.split('.')[0]\n",
    "    \n",
    "    #df.write.mode('overwrite').parquet('/user/hive/warehouse/' + clear_name + '.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e3273",
   "metadata": {},
   "source": [
    "# Вариант чтения 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "struct = StructType([\n",
    "    StructField(\"tranaction_id\", IntegerType(), nullable = True),\n",
    "    StructField(\"tx_datetime\", StringType(), nullable = True), \n",
    "    StructField(\"customer_id\", IntegerType(), nullable = True), \n",
    "    StructField(\"terminal_id\", IntegerType(), nullable = True),\n",
    "    StructField(\"tx_amount\", DoubleType(), nullable = True), \n",
    "    StructField(\"tx_time_seconds\", IntegerType(), nullable = True), \n",
    "    StructField(\"tx_time_days\", IntegerType(), nullable = True),\n",
    "    StructField(\"tx_fraud\", IntegerType(), nullable = True), \n",
    "    StructField(\"tx_fraud_scenario\", IntegerType(), nullable = True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(name):\n",
    "    df = spark.read\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .option(\"inferSchema\", \"false\")\\\n",
    "          .option(\"delimiter\", \",\")\\\n",
    "          .schema(struct)\\\n",
    "          .csv(\"s3a://hw2-mlops-course/\" + name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e18ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a3c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f2540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9682b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
